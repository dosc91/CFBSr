% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_chunk_duration.R
\name{get_chunk_duration}
\alias{get_chunk_duration}
\title{Compute chunk boundaries based on the Hilbert amplitude envelope}
\usage{
get_chunk_duration(
  chopped_dir,
  window = 1000,
  smooth = 800,
  kernel = "daniell",
  plot = FALSE,
  fun = "min",
  progress = TRUE
)
}
\arguments{
\item{chopped_dir}{Directory of the individual sound files. Typically created with \code{get_word_sound_files}.}

\item{window}{Size of the rolling window to apply the \code{fun}ction to.}

\item{smooth}{The parameter controlling the bandwidth of the kernel. Defaults to \code{800}.}

\item{kernel}{The type of kernel. Defaults to \code{"daniell"}.}

\item{plot}{Whether a plot should be created for each Hilbert envelope. Defaults to \code{FALSE}. Note that plotting takes a substantial amount of time.}

\item{fun}{Function to identify chunk boundaries by. Defaults to \code{"min"} following Arnold et al. (2017). Alternatively takes \code{max}, following Shafaei-Bajestan et al. (2023).}

\item{progress}{Show a console progress bar. Defaults to \code{TRUE}.}
}
\value{
A list object.
}
\description{
This function computes chunk boundaries from audio signals based on the Hilbert amplitude envelope.
}
\examples{
\dontrun{chunk_durations <- get_chunk_duration(chopped_dir = "C:/Users/Project/chopped", fun = "min")}

}
\references{
Arnold, D. (2018). AcousticNDLCodeR: Coding Sound Files for Use with NDL. R package version 1.0.2. Retrieved from https://CRAN.R-project.org/package=AcousticNDLCodeR

Arnold, D., Tomaschek, F., Sering, K., Lopez, F., & Baayen, R. H. (2017). Words from spontaneous conversational speech can be recognized with human-like accuracy by an error-driven learning algorithm that discriminates between meanings straight from smart acoustic features, bypassing the phoneme as recognition unit. PLOS ONE, 12(4), e0174623. https://doi.org/10.1371/journal.pone.0174623

Ligges, U., Krey, S., Mersmann, O., & Schnackenberg, S. (2023). tuneR: Analysis of Music and Speech. Retrieved from https://CRAN.R-project.org/package=tuneR

Shafaei-Bajestan, E., Moradipour-Tari, M., Uhrig, P., & Baayen, R. H. (2023). LDL-AURIS: a computational model, grounded in error-driven learning, for the comprehension of single spoken words. Language, Cognition and Neuroscience, 38(4), 509â€“536. https://doi.org/10.1080/23273798.2021.1954207

Sueur, J., Aubin, T., & Simonis, C. (2008). Seewave: a free modular tool for sound analysis and synthesis. Bioacoustics, 18(3), 213-226.

Zeileis, A., & Grothendieck, G. (2005). zoo: S3 Infrastructure for Regular and Irregular Time Series. Journal of Statistical Software, 14(6), 1-27. https://doi.org/10.18637/jss.v014.i06
}
\author{
Dominic Schmitz
}
